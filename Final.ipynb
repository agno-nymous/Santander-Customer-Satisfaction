{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[data](https://www.kaggle.com/c/santander-customer-satisfaction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib #for loading pickle files\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics,ensemble,model_selection,linear_model,tree,calibration,cluster\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "import scipy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import itertools\n",
    "np.random.seed(13154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_variance(train,test):\n",
    "    \"\"\"\n",
    "    removing all features with null variance/zero variance\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for col in train.columns: \n",
    "        if train[col].var()==0:\n",
    "            i+=1\n",
    "            del train[col]\n",
    "            del test[col]\n",
    "    print(\"%i features were found to have zero variance and these were all removed.\"%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sparse(train,test):\n",
    "    \"\"\"\n",
    "    remove sparse features (features that have 99th percentile value = 0)\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for col in train.columns: #reomving all sparse features\n",
    "        if np.percentile(train[col],99)==0:\n",
    "            i+=1\n",
    "            del train[col]\n",
    "            del test[col]\n",
    "    print(\"%i features were found to be sparse and these were all removed.\"%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_identical_columns(train,test):\n",
    "    \"\"\"\n",
    "    remove columns that have identical values\n",
    "    \"\"\"\n",
    "    combinations = list(itertools.combinations(train.columns,2)) #getting all the combinations of columns\n",
    "    remove=[]\n",
    "    for f1,f2 in combinations: #finding all columns which have same datavalues, they don't share same feature name\n",
    "        if (f1 not in remove) & (f2 not in remove):\n",
    "            if train[f1].equals(train[f2]):\n",
    "                remove.append(f1)\n",
    "    train.drop(remove,axis=1,inplace=True)\n",
    "    test.drop(remove,axis=1,inplace=True)\n",
    "    print(\"%i were found to be duplicated columns and %i of these were removed.\"%(len(remove)*2,len(remove)))\n",
    "    del remove\n",
    "    del combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_test):\n",
    "    \"\"\"\n",
    "    function which takes an input and computes preprocessing\n",
    "    \"\"\"\n",
    "    X_train = pd.read_csv('./train.csv')\n",
    "    #preprocessing\n",
    "    remove_null_variance(train=X_train,test=X_test)\n",
    "    remove_sparse(train=X_train,test=X_test)\n",
    "    remove_identical_columns(train=X_train,test=X_test)\n",
    "\n",
    "    X_train.to_pickle(\"./preprocessed_train.pkl\")\n",
    "    X_test.to_pickle(\"./preprocessed_test.pkl\")\n",
    "    print(\"Preprocessing completed\")\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_var15_below_23(train,test):\n",
    "    \"\"\"\n",
    "    create a new feature which tells whether a customer is below 23 years old or not\n",
    "    \"\"\"\n",
    "    print(\"Creating 'var15_below_23' feature\")\n",
    "    for df in [train,test]:\n",
    "        df['var15_below_23'] = np.zeros(df.shape[0],dtype=int)\n",
    "        df.loc[df['var15']<23,'var15_below_23']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_var15(train,test):\n",
    "    \"\"\"\n",
    "    binning var15 feature into 5 bins\n",
    "    \"\"\"\n",
    "    print(\"binning 'var15' feature\")\n",
    "    _,bins = pd.cut(train['var15'].values,5,retbins=True) #getting the bins\n",
    "    train['var15'] = pd.cut(train['var15'].values,bins,labels=False)\n",
    "    test['var15'] = pd.cut(test['var15'].values,bins,labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_no_zeros(train,test):\n",
    "    \"\"\"\n",
    "    Add feature which tells the no. of occurences of zeros,nonzeros across a datapoint\n",
    "    \"\"\"\n",
    "    print(\"Creating 'no_zeros' and 'no_nonzeroes' feature\")\n",
    "    col = [k for k in train.columns if k!='TARGET']\n",
    "    for df in [train,test]:\n",
    "        df['no_zeros'] = (df.loc[:,col]==0).sum(axis=1).values\n",
    "        df['no_nonzeros'] = (df.loc[:,col]!=0).sum(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_no_zeros_keyword(keyword,train,test):\n",
    "    \"\"\"\n",
    "    Add feature which tells the no. of occurences of zeros,nonzeros across a datapoint\n",
    "    for a specific keyword\n",
    "    \"\"\"\n",
    "    print(\"Creating 'no_zeros' and 'no_nonzeroes' feature for %s keyword\"%(keyword))\n",
    "    col = [k for k in train.columns if keyword in k]\n",
    "    for df in [train,test]:\n",
    "        df['no_zeros_'+keyword] = (df.loc[:,col]==0).sum(axis=1).values\n",
    "        df['no_nonzeros_'+keyword] = (df.loc[:,col]!=0).sum(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_col(col,features,train,test):\n",
    "  \"\"\"\n",
    "  Gets the average numerical values of features for each category in 'col' feature\n",
    "  and add it as feature \n",
    "  \"\"\"\n",
    "  print(\"Creating average numerical values feature\")\n",
    "  if \"train_average.pkl\" not in os.listdir('./'): #less time\n",
    "    for df in [train,test]:\n",
    "        unique_values = df[col].unique()\n",
    "        for feature in features:\n",
    "          avg_value=[]\n",
    "          for value in unique_values:\n",
    "            avg = df.loc[df[col]==value,feature].mean() #taking average for each category for feature col\n",
    "            avg_value.append(avg)\n",
    "          avg_dict = dict(zip(unique_values,avg_value))\n",
    "          new_col = 'avg_'+col+'_'+feature\n",
    "          df[new_col] = np.zeros(df.shape[0])\n",
    "          for value in unique_values:\n",
    "            df.loc[df[col]==value,new_col] = avg_dict[value]\n",
    "    df.to_pickle(\"./train_average.pkl\")\n",
    "  else:\n",
    "    for df in [test]:\n",
    "      unique_values = df[col].unique()\n",
    "      for feature in features:\n",
    "        avg_value=[]\n",
    "        for value in unique_values:\n",
    "          avg = df.loc[df[col]==value,feature].mean() #taking average for each category for feature col\n",
    "          avg_value.append(avg)\n",
    "        avg_dict = dict(zip(unique_values,avg_value))\n",
    "        new_col = 'avg_'+col+'_'+feature\n",
    "        df[new_col] = np.zeros(df.shape[0])\n",
    "        for value in unique_values:\n",
    "          df.loc[df[col]==value,new_col] = avg_dict[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdzation(train,test):\n",
    "  \"\"\"\n",
    "  apply standardization to features\n",
    "  for train and test using scaler\n",
    "  for all columns\n",
    "  \"\"\"\n",
    "  print(\"Applying standardization\")\n",
    "  filepath = './scaler.pkl'\n",
    "  col=[i for i in train.columns if (i!='TARGET') & (i!='ID')]\n",
    "  with open(filepath,'rb') as f:\n",
    "    scaler = joblib.load(f)\n",
    "  train.loc[:,col]= scaler.transform(train.loc[:,col])\n",
    "  test.loc[:,col] = scaler.transform(test.loc[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kmeans_cluster(train,test,ncluster=[2,4,6,8,10]):\n",
    "    \"\"\"\n",
    "    Function add Kmeans cluster value as features for n in ncluster\n",
    "    \"\"\"\n",
    "    print('adding kmeans cluster features')\n",
    "    X_tr = train.drop(['TARGET','ID'],axis=1).copy()\n",
    "    X_te = test.drop(['ID'],axis=1).copy()\n",
    "    if 'train_cluster.pkl' not in os.listdir('./'):\n",
    "      for n in ncluster:\n",
    "          print(\"for n=%i:\"%(n))\n",
    "          filepath = 'kmeans_'+str(n)+'.pkl' #model pkl\n",
    "          feat_name = 'Kmeans_'+str(n)\n",
    "          with open(filepath,'rb') as f:\n",
    "            kmeans = joblib.load(f)\n",
    "          train[feat_name] = kmeans.predict(X_tr)\n",
    "          test[feat_name] = kmeans.predict(X_te)\n",
    "          train.to_pickle('./train_cluster.pkl')\n",
    "    else:\n",
    "      for n in ncluster:\n",
    "          print(\"for n=%i:\"%(n))\n",
    "          feat_name = 'Kmeans_'+str(n)\n",
    "          filepath = 'kmeans_'+str(n)+'.pkl'\n",
    "          with open(filepath,'rb') as f:\n",
    "            kmeans = joblib.load(f)\n",
    "          test[feat_name] = kmeans.predict(X_te)\n",
    "      train = pd.read_pickle('./train_cluster.pkl')\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corr_var(train,test,target_threshold = 10**-3,within_threshold=0.95):\n",
    "  \"\"\"\n",
    "  Remove correlated features that have low correlation with target \n",
    "  and have high correlation with each other (keeping one)\n",
    "  \"\"\"\n",
    "  print(\"Removing features based on correlation and variance\")\n",
    "  #removing all low correlated variables with target\n",
    "  initial_feature = train.shape[1]\n",
    "  corr = train.drop(\"ID\",axis=1).corr().abs()\n",
    "  corr_target = pd.DataFrame(corr['TARGET']).sort_values(by='TARGET')\n",
    "  threshold=target_threshold\n",
    "  feat_df =corr_target[(corr_target['TARGET'])<=threshold]\n",
    "  print(\"There are %i features that have a correlation values less than %.3f with 'TARGET'. We will remove all of this.\"\\\n",
    "        %(feat_df.shape[0],threshold))\n",
    "  print(\"Removing.........\")\n",
    "  \n",
    "  for df in [train,test]:\n",
    "    df.drop(feat_df.index,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  #reomving highly correlated features(keeping one)\n",
    "  #https://www.dezyre.com/recipes/drop-out-highly-correlated-features-in-python\n",
    "  corr.drop('TARGET',axis=1,inplace=True)\n",
    "  corr.drop('TARGET',axis=0,inplace=True)\n",
    "  corr.drop(feat_df.index,axis=1,inplace=True)\n",
    "  corr.drop(feat_df.index,inplace=True)\n",
    "  threshold = within_threshold\n",
    "  upper = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool)) #getting upper traingle of correlation matrix\n",
    "  column = [col for col in upper.columns if any(upper[col]>threshold)] #getting all columns that have high correlation with one of the features\n",
    "  print(\"There are %i features that have high correlation with another feature with threshold being kept as %.3f and above. We will remove all of this.\"\\\n",
    "        %(len(column),threshold))\n",
    "  print(\"Removing.........\")\n",
    "  for df in [train,test]:\n",
    "    df.drop(column,axis=1,inplace=True)\n",
    "\n",
    "  print(\"The features were changed from %i to %i. %i features were removed.\"%(initial_feature,train.shape[1],initial_feature-train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log(train,test,column):\n",
    "  \"\"\"\n",
    "  apply log transformation to all features in col variable\n",
    "  \"\"\"\n",
    "  tr = train.copy()\n",
    "  te = test.copy()\n",
    "  for df in [tr,te]:\n",
    "    for col in column:\n",
    "      df.loc[df[col]>=0,col] = np.log(df.loc[df[col]>=0,col].values)\n",
    "\n",
    "  return tr,te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_encoding(df,test_df,column,target='TARGET',alpha=5000):\n",
    "        \"\"\"\n",
    "        Here we are using response encoding with laplace smoothing to a categorical column\n",
    "        and transform the respective column in train,test,val datasets.\n",
    "        Here we will be repeating the values of each category alpha times.\n",
    "        \"\"\"\n",
    "        feature = column+'_1'\n",
    "        feature_ = column + '_0'\n",
    "        unique_values = set(df[column].values) #all unique values in that categorical column\n",
    "        dict_values = {} #storing the response encoding values for target=1\n",
    "        dict_values_ = {} #storing the response encoding values for target=0\n",
    "        for value in unique_values:\n",
    "            total = len(df[df[column]==value]) #the total no. of datapoints with 'value' catgeory\n",
    "            sum_promoted = len(df[(df[column]==value) & df[target]==1]) #no. of all datapoints with category being 'value' and target==1\n",
    "            sum_unpromoted = total-sum_promoted #no. of all datapoints with category being 'value' and target==0\n",
    "            dict_values[value] = np.round((sum_promoted+alpha)/(total+alpha*len(unique_values)),2) #storing the obtained result in a dictionary\n",
    "            dict_values_[value] = np.round((sum_unpromoted+alpha)/(total+alpha*len(unique_values)),2)\n",
    "        dict_values['unknown']=0.5 #unknown categories that are not seen in train will be assigned a score of 0.5\n",
    "        dict_values_['unknown'] = 0.5\n",
    "        df[feature]=(df[column].map(dict_values)).values\n",
    "        df[feature_] = (df[column].map(dict_values_)).values\n",
    "        df.drop(column,axis=1,inplace=True)\n",
    "    \n",
    "        unique_values_test = set(test_df[column].values)\n",
    "        test_df[column]=test_df[column].apply(lambda x: 'unknown' if x in (unique_values_test-unique_values) else x )\n",
    "        test_df[feature] = (test_df[column].map(dict_values)).values\n",
    "        test_df[feature_] = (test_df[column].map(dict_values_)).values\n",
    "        test_df.drop(column,axis=1,inplace=True) \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pca_features(train,test,n=2):\n",
    "  \"\"\"\n",
    "  Add pca values as features with n resulting components\n",
    "  \"\"\"\n",
    "  pca = sklearn.decomposition.PCA(n_components=n)\n",
    "  feat_names = ['pca_'+str(i) for i in range(n)]\n",
    "\n",
    "  #train\n",
    "  X= train.drop([\"ID\",\"TARGET\"],axis=1)\n",
    "  with open('./pca.pkl','rb') as f:\n",
    "    pca = joblib.load(f)\n",
    "  X_embedded=pca.transform(X)\n",
    "  for i,feat in enumerate(feat_names):\n",
    "    train[feat] = X_embedded[:,i]\n",
    "\n",
    "  #test\n",
    "  X= test.drop(\"ID\",axis=1)\n",
    "  X_embedded=pca.transform(X)\n",
    "  for i,feat in enumerate(feat_names):\n",
    "    test[feat] = X_embedded[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X_test):\n",
    "    \"\"\"\n",
    "    Computes feature engineering\n",
    "    \"\"\"\n",
    "    files = os.listdir('./')\n",
    "\n",
    "    #creating features\n",
    "    X_train = pd.read_pickle('./preprocessed_train.pkl')\n",
    "\n",
    "    create_var15_below_23(train=X_train,test=X_test)\n",
    "\n",
    "    bin_var15(train=X_train,test=X_test)\n",
    "\n",
    "    add_feature_no_zeros(train=X_train,test=X_test)\n",
    "\n",
    "    keywords = ['saldo' , 'ind', 'num', 'imp']\n",
    "    for k in keywords:\n",
    "        add_feature_no_zeros_keyword(k,X_train,X_test)\n",
    "\n",
    "    #we will be taking average columns for saldo and imp with categorical columns being the ones that have between 50 and 210 unqiue values\n",
    "    features = [i for i in X_train.columns if ('saldo' in i) & ('no_zeros' not in i)]\n",
    "    features.extend([i for i in X_train.columns if ('imp' in i) & ('no_zeros' not in i)])\n",
    "    columns = [i for i in X_train.columns if (X_train[i].nunique()<=210) & (X_train[i].nunique()>50)] #categorical features\n",
    "    for col in tqdm(columns):\n",
    "        average_col(col,features,X_train,X_test)\n",
    "    if 'train_average.pkl' not in os.listdir('./'):\n",
    "        X_train.to_pickle('./train_average.pkl')\n",
    "    else:\n",
    "        X_train = pd.read_pickle(\"train_average.pkl\")\n",
    "    stdzation(X_train,X_test)\n",
    "    X_train,X_test = add_kmeans_cluster(train=X_train,test=X_test)\n",
    "    remove_corr_var(train=X_train,test=X_test)\n",
    "\n",
    "    #log transformation to saldo and imp features\n",
    "    features = [i for i in X_train.columns if (('saldo' in i)|('imp' in i))&((X_train[i].values>=0).all())]\n",
    "    X_train,X_test = apply_log(X_train,X_test,features)\n",
    "\n",
    "    #response encoding columns which have 2-10 unique values\n",
    "    cat_col = []\n",
    "    for col in X_train.columns:\n",
    "        if (X_train[col].nunique()<=10) & (col!='TARGET') & (X_train[col].nunique()>2) & ('Kmeans' not in col):\n",
    "            cat_col.append(col)\n",
    "\n",
    "    alpha=100\n",
    "    for col in tqdm(cat_col):\n",
    "        response_encoding(X_train,X_test,col,alpha=alpha)\n",
    "        \n",
    "        \n",
    "    print(\"adding pca features to the datasets\")\n",
    "    add_pca_features(X_train,X_test) #adding pca feature to log response encoded data\n",
    "    print(\"Feature Engineering done.....\")\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(df,feat_imp,top=50):\n",
    "  \"\"\"\n",
    "  gets the top features.\n",
    "  if top is of integer dtype it will return top features,\n",
    "  if top is between 0 and 1, it will return features that have at least feature importance value= top\n",
    "  \"\"\"\n",
    "  if (top>0.)&(top<=1.): #getting the features that have feature importance value greater than top\n",
    "    feature_to_consider = [1 if i>=top else 0 for i in feat_imp]\n",
    "    most_important_feat = [i  for i,j in zip(df.columns,feature_to_consider) if (j==1)]#getiing the columns names\n",
    "  else:\n",
    "    top_indices = np.argsort(feat_imp)[::-1][:top] #getting the indices with top feature importace\n",
    "    most_important_feat = df.columns[top_indices] #getiing the columns names\n",
    "  return most_important_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features_log(X_train,X_test,y_train):\n",
    "    \"\"\"\n",
    "    Gets the top 250 features for log transformed data\n",
    "    \"\"\"\n",
    "    if 'log_train_re_250.pkl' not in os.listdir('./'):\n",
    "        X_train1,X_val,y_tr,y_val = model_selection.train_test_split(X_train,\n",
    "                                                                         y_train,\n",
    "                                                                         stratify=y_train,\n",
    "                                                                         test_size=0.15)\n",
    "        filepath = 'top_feature_log.pkl'\n",
    "        with open(filepath,'rb') as f:\n",
    "            model = joblib.load(filepath)        \n",
    "        features = get_top_features(X_train,model.feature_importances_,top=250)\n",
    "        X_train_250 = X_train.loc[:,features]\n",
    "        X_test_250 = X_test.loc[:,features]\n",
    "        X_train_250.to_pickle('./log_train_re_250.pkl')\n",
    "    else:\n",
    "        X_train_250 = X_train.loc[:,pd.read_pickle('./log_train_re_250.pkl').columns]\n",
    "        X_test_250 = X_test.loc[:,X_train_250.columns]\n",
    "\n",
    "    return X_train_250,X_test_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling_log(X_train,X_test,y_train):\n",
    "    \"\"\"\n",
    "    return y_test for log top 250 dataset \n",
    "    \"\"\"\n",
    "    filepath = './model_log.pkl'\n",
    "    with open(filepath,'rb') as file:\n",
    "        model = joblib.load(file)\n",
    "    y_test = model.predict_proba(X_test)[:,1]\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features_normal(X_train,X_test,y_train):\n",
    "    \"\"\"\n",
    "    Gets the top 250 features for normal transformed data\n",
    "    \"\"\"\n",
    "    if 'normal_train_re_250.pkl' not in os.listdir('./'):\n",
    "        X_train1,X_val,y_tr,y_val = model_selection.train_test_split(X_train,\n",
    "                                                                         y_train,\n",
    "                                                                         stratify=y_train,\n",
    "                                                                         test_size=0.15)\n",
    "        with open(filepath,'rb') as f:\n",
    "            model = joblib.load(filepath)        \n",
    "        features = get_top_features(X_train,model.feature_importances_,top=250)\n",
    "        X_train_250 = X_train.loc[:,features]\n",
    "        X_test_250 = X_test.loc[:,features]\n",
    "        X_train_250.to_pickle('./normal_train_re_250.pkl')\n",
    "    else:\n",
    "        X_train_250 = X_train.loc[:,pd.read_pickle('./normal_train_re_250.pkl').columns]\n",
    "        X_test_250 = X_test.loc[:,X_train_250.columns]\n",
    "    \n",
    "    return X_train_250,X_test_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling_normal(X_train,X_test,y_train):\n",
    "    \"\"\"\n",
    "    return y_test for normal top 250 dataset \n",
    "    \"\"\"\n",
    "    filepath = './model_normal.pkl'\n",
    "    with open(filepath,'rb') as file:\n",
    "        model = joblib.load(file)\n",
    "    y_test = model.predict_proba(X_test)[:,1]\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final1(test):\n",
    "    \"\"\"\n",
    "    returns y_pred if raw input file is given\n",
    "    \"\"\"\n",
    "    #preprocessing\n",
    "    train,test = preprocessing(test)\n",
    "    \n",
    "    #feature engineering\n",
    "    train,test = feature_engineering(test)\n",
    "    \n",
    "    #get top 250 features\n",
    "    X_train = train.drop(['ID','TARGET'],axis=1)\n",
    "    y_train = train['TARGET'].values\n",
    "    X_test = test.drop('ID',axis=1)\n",
    "    del train,test\n",
    "    X_train1,X_test1 = top_features_log(X_train,X_test,y_train) #get top 250 features for log\n",
    "    X_train2,X_test2 = top_features_normal(X_train,X_test,y_train) #get top 250 features for normal\n",
    "    del X_train,X_test\n",
    "    #modelling log\n",
    "    y_pred1 = modelling_log(X_train1,X_test1,y_train)\n",
    "    y_pred2 = modelling_normal(X_train2,X_test2,y_train)\n",
    "    y_pred =(y_pred1+y_pred2)/2 #simple ensembling\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final2(test,y_test):\n",
    "    \"\"\"\n",
    "    returns auc score when test and y_test are given\n",
    "    here test will contain \"TARGET\" (raw input file)\n",
    "    \"\"\"\n",
    "    #preprocessing\n",
    "    train,test = preprocessing(test)\n",
    "    \n",
    "    #feature engineering\n",
    "    train,test = feature_engineering(test)\n",
    "    \n",
    "    #get top 250 features\n",
    "    X_train = train.drop(['ID','TARGET'],axis=1)\n",
    "    y_train = train['TARGET'].values\n",
    "    X_test = test.drop('ID',axis=1)\n",
    "    del train,test\n",
    "    X_train1,X_test1 = top_features_log(X_train,X_test,y_train) #get top 250 features for log\n",
    "    X_train2,X_test2 = top_features_normal(X_train,X_test,y_train) #get top 250 features for normal\n",
    "    del X_train,X_test\n",
    "    \n",
    "    y_pred1 = modelling_log(X_train1,X_test1,y_train) #modelling log\n",
    "    y_pred2 = modelling_normal(X_train2,X_test2,y_train) #modelling normal\n",
    "    y_pred =(y_pred1+y_pred2)/2 #simple ensembling\n",
    "    \n",
    "    #score\n",
    "    auc_score = metrics.roc_auc_score(y_test,y_pred)\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testy(y_test):\n",
    "  \"\"\"\n",
    "  creates submission test file csv\n",
    "  \"\"\"\n",
    "  test = X_test_l\n",
    "  feat = test.columns\n",
    "  feat_to_drop = set(test.columns)-{'ID'}\n",
    "  test.drop(feat_to_drop,axis=1,inplace=True)\n",
    "  test['TARGET'] = y_test\n",
    "  test.to_csv('./submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "train = pd.read_csv('./train.csv')\n",
    "y_train = train['TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features were found to have zero variance and these were all removed.\n",
      "188 features were found to be sparse and these were all removed.\n",
      "12 were found to be duplicated columns and 6 of these were removed.\n",
      "Preprocessing completed\n",
      "Creating 'var15_below_23' feature\n",
      "binning 'var15' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for saldo keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for ind keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for num keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for imp keyword\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▎                                                                              | 1/19 [00:21<06:34, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                          | 2/19 [00:33<05:17, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████                                                                      | 3/19 [00:45<04:29, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▍                                                                 | 4/19 [01:02<04:14, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▊                                                             | 5/19 [01:18<03:51, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                        | 6/19 [01:39<03:52, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▌                                                    | 7/19 [01:57<03:35, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▉                                                | 8/19 [02:16<03:21, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [02:41<03:21, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [03:03<03:08, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [03:31<03:02, 22.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [03:59<02:50, 24.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [04:26<02:32, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [04:58<02:15, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [05:37<02:03, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [06:13<01:37, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [06:51<01:08, 34.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [07:25<00:33, 33.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [08:01<00:00, 25.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization\n",
      "adding kmeans cluster features\n",
      "for n=2:\n",
      "for n=4:\n",
      "for n=6:\n",
      "for n=8:\n",
      "for n=10:\n",
      "Removing features based on correlation and variance\n",
      "There are 46 features that have a correlation values less than 0.001 with 'TARGET'. We will remove all of this.\n",
      "Removing.........\n",
      "There are 553 features that have high correlation with another feature with threshold being kept as 0.950 and above. We will remove all of this.\n",
      "Removing.........\n",
      "The features were changed from 957 to 358. 599 features were removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pca features to the datasets\n",
      "Feature Engineering done.....\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%time y_test = final1(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05192093, 0.04664471, 0.002612  , ..., 0.00242902, 0.03230181,\n",
       "       0.00225586], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features were found to have zero variance and these were all removed.\n",
      "188 features were found to be sparse and these were all removed.\n",
      "12 were found to be duplicated columns and 6 of these were removed.\n",
      "Preprocessing completed\n",
      "Creating 'var15_below_23' feature\n",
      "binning 'var15' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for saldo keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for ind keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for num keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for imp keyword\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▎                                                                              | 1/19 [00:22<06:50, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                          | 2/19 [00:31<05:16, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████                                                                      | 3/19 [00:43<04:26, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▍                                                                 | 4/19 [01:00<04:10, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▊                                                             | 5/19 [01:16<03:49, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                        | 6/19 [01:36<03:46, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▌                                                    | 7/19 [01:55<03:34, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▉                                                | 8/19 [02:14<03:21, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [02:37<03:18, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [02:59<03:04, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [03:25<02:57, 22.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [03:53<02:46, 23.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [04:20<02:28, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [04:52<02:14, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [05:32<02:03, 30.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [06:10<01:38, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [06:56<01:13, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [07:38<00:38, 38.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [08:19<00:00, 26.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization\n",
      "adding kmeans cluster features\n",
      "for n=2:\n",
      "for n=4:\n",
      "for n=6:\n",
      "for n=8:\n",
      "for n=10:\n",
      "Removing features based on correlation and variance\n",
      "There are 46 features that have a correlation values less than 0.001 with 'TARGET'. We will remove all of this.\n",
      "Removing.........\n",
      "There are 553 features that have high correlation with another feature with threshold being kept as 0.950 and above. We will remove all of this.\n",
      "Removing.........\n",
      "The features were changed from 957 to 358. 599 features were removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:23<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pca features to the datasets\n",
      "Feature Engineering done.....\n",
      "Wall time: 17min 42s\n"
     ]
    }
   ],
   "source": [
    "X_test = train.drop(['TARGET'],axis=1)\n",
    "y_test = y_train\n",
    "%time auc_score = final2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8879187176022363"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "train = pd.read_csv('./train.csv')\n",
    "y_train = train['TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features were found to have zero variance and these were all removed.\n",
      "188 features were found to be sparse and these were all removed.\n",
      "12 were found to be duplicated columns and 6 of these were removed.\n",
      "Preprocessing completed\n",
      "Creating 'var15_below_23' feature\n",
      "binning 'var15' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for saldo keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for ind keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for num keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for imp keyword\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▎                                                                              | 1/19 [00:00<00:03,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n",
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                          | 2/19 [00:00<00:04,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████                                                                      | 3/19 [00:01<00:05,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▍                                                                 | 4/19 [00:01<00:05,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▊                                                             | 5/19 [00:02<00:05,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                        | 6/19 [00:02<00:06,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▌                                                    | 7/19 [00:03<00:06,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▉                                                | 8/19 [00:03<00:05,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [00:04<00:04,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [00:04<00:03,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [00:04<00:03,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [00:05<00:03,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [00:05<00:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [00:06<00:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [00:07<00:02,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [00:08<00:02,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [00:09<00:01,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [00:11<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:12<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization\n",
      "adding kmeans cluster features\n",
      "for n=2:\n",
      "for n=4:\n",
      "for n=6:\n",
      "for n=8:\n",
      "for n=10:\n",
      "Removing features based on correlation and variance\n",
      "There are 46 features that have a correlation values less than 0.001 with 'TARGET'. We will remove all of this.\n",
      "Removing.........\n",
      "There are 553 features that have high correlation with another feature with threshold being kept as 0.950 and above. We will remove all of this.\n",
      "Removing.........\n",
      "The features were changed from 957 to 358. 599 features were removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:12<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pca features to the datasets\n",
      "Feature Engineering done.....\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%time y_test = final1(test.iloc[:25,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:100] # class imbalance making sure 2 classs are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features were found to have zero variance and these were all removed.\n",
      "188 features were found to be sparse and these were all removed.\n",
      "12 were found to be duplicated columns and 6 of these were removed.\n",
      "Preprocessing completed\n",
      "Creating 'var15_below_23' feature\n",
      "binning 'var15' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for saldo keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for ind keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for num keyword\n",
      "Creating 'no_zeros' and 'no_nonzeroes' feature for imp keyword\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▎                                                                              | 1/19 [00:00<00:06,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                          | 2/19 [00:00<00:07,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████                                                                      | 3/19 [00:02<00:13,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▍                                                                 | 4/19 [00:03<00:13,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▊                                                             | 5/19 [00:04<00:11,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                        | 6/19 [00:05<00:10,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▌                                                    | 7/19 [00:06<00:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▉                                                | 8/19 [00:06<00:09,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [00:07<00:08,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [00:08<00:07,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [00:09<00:06,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [00:10<00:06,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [00:11<00:05,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [00:12<00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [00:14<00:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [00:17<00:05,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [00:20<00:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [00:22<00:02,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average numerical values feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization\n",
      "adding kmeans cluster features\n",
      "for n=2:\n",
      "for n=4:\n",
      "for n=6:\n",
      "for n=8:\n",
      "for n=10:\n",
      "Removing features based on correlation and variance\n",
      "There are 46 features that have a correlation values less than 0.001 with 'TARGET'. We will remove all of this.\n",
      "Removing.........\n",
      "There are 553 features that have high correlation with another feature with threshold being kept as 0.950 and above. We will remove all of this.\n",
      "Removing.........\n",
      "The features were changed from 957 to 358. 599 features were removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:12<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pca features to the datasets\n",
      "Feature Engineering done.....\n",
      "Wall time: 5min 35s\n"
     ]
    }
   ],
   "source": [
    "X_test = train[:100].drop(['TARGET'],axis=1)\n",
    "y_test = y_train[:100]\n",
    "%time auc_score = final2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091836734693877"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
